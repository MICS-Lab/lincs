.. Copyright 2023 Vincent Jacques

==========
User Guide
==========

Before you read this document, we strongly recommend you read our :doc:`conceptual overview <conceptual-overview>` as it establishes the bases for this guide.
We also recommend you follow our :doc:`"Get started" guide <get-started>` to get a first hands-on experience with *lincs*.


Formatting data for *lincs*
===========================

*lincs* manipulates files for three types of data.

.. _user-file-problem:

"Problem" files
---------------

.. highlight:: yaml

The concept of classification problem is defined in our :ref:`conceptual overview <overview-about-classification>`.
To describe problems, *lincs* uses YAML files conforming to the `JSON schema <https://json-schema.org/>`_ you'll find in our :ref:`reference documentation <ref-file-problem>`.

Here is an example of a problem file::

    {{ notebooks["integration-tests/file-formats/file-formats.ipynb"]["cells"][0]["outputs"][0]["text"] | join('    ') }}

The two first keys, ``kind`` and ``format_version`` are here to identify exactly the file format.
For now, they must always be set to ``classification-problem`` and ``1`` respectively.

Criteria
^^^^^^^^

The third key, ``criteria``, is a list of the descriptions of the criteria of the problem.
This list must contain at least one element because classification problems must have at least one criterion.

Each criterion has a ``name``.

Currently, criteria can only take floating point values, so their ``value_type`` is always ``real``.
We expect this could evolve to also support criteria with integer or explicitly enumerated values.

Then, the ``preference_direction`` key describe what makes "good values" for this criterion.
If it is ``increasing`` (resp. ``decreasing``), then higher (resp. lower) numerical values correspond to upper categories.
Note that this preference direction comes from expert knowledge about the structure of the problem,
and will be used as an absolute truth when learning a model for this problem.
We expect the supported preference directions could evolve to also support single-peaked criteria,
where intermediate numerical value correspond to upper categories, and extreme values to lower categories.
We also expect this could evolve to support criteria with unknown preference direction,
to support the case where no expert knowledge is available and delegate this choice to the learning process.

Finally, for criteria with numerical ``value_type`` (currently all of them),
the ``min_value`` and ``max_value`` keys describe the range of values the criterion can take.

Categories
^^^^^^^^^^

The fourth key in the problem file, ``categories``, is a list of the descriptions of the categories of the problem.
It must contain at least two elements because classification problems must have at least two categories.

It must be sorted in increasing order: lower categories first and upper categories last.

Its elements are relatively simple as they only get a ``name``.

.. _user-file-ncs-model:

"Model" files
-------------

The concept of NCS classification model is defined in our :ref:`conceptual overview <overview-ncs>`.
To describe models, *lincs* uses YAML files conforming to the JSON schema you'll find in our :ref:`reference documentation <ref-file-ncs-model>`.

A model file is always associated to a problem file because a model exists only relatively to a given problem.
This fact is not captured by our file format for technical reasons:
embedding the problem in the model file would lead to unwanted repetitions,
referencing the problem file by name would not be robust because files can be renamed,
and referencing the problem file by content (using a hash) would forbid any change in the problem file.
So it's the user's responsibility to keep track of that information and always give *lincs* the correct problem file along with a model file.

Here is an example of a model file corresponding to the problem file above::

    {{ notebooks["integration-tests/file-formats/file-formats.ipynb"]["cells"][1]["outputs"][0]["text"] | join('    ') }}

Like for problem files, the two first keys must take exactly these values.

Accepted values
^^^^^^^^^^^^^^^

The third key, ``accepted_values``, lists the descriptions of the accepted values according to each criterion of the problem.
It must contain exactly as many elements as the ``criteria`` list in the problem file.

For NCS models as currently defined in our conceptual overview, accepted values are simply above a profile.
The profile is a list of thresholds, one for each criterion, that separates two categories.
But this lacks generality, and we expect this could evolve, for example for single-peaked criteria.
For such a criterion, the determination of the accepted values will require two limits (upper and lower) instead of just one threshold.

So our file format takes an transposed approach and focusses on criteria instead of profiles:
for each criterion, it describes the method used to accept values at different category levels.

For current criteria (with ``increasing`` or ``decreasing`` preference direction), the method is always ``kind: thresholds``,
and the ``thresholds`` attribute lists the successive values required to enter an upper category.
It must have as many elements as there are boundaries between categories, *i.e.* as there are categories minus one.
It's always sorted, in increasing order for ``increasing`` criteria and in decreasing order for ``decreasing`` criteria.

Note that this list is not a profile: it does not describe the limits between categories.
The matrix made of these lists is the transposed of the matrix made of the profiles.

When we support single-peaked criteria or criteria with unknown preference direction,
we'll introduce other ``kinds`` of accepted values with new attributes instead of ``thresholds``.

==================================  ========================  ==========================
Criterion ``preference_direction``  Accepted values ``kind``  Accepted values attributes
==================================  ========================  ==========================
``increasing``                      ``thresholds``            ``thresholds``
``decreasing``                      ``thresholds``            ``thresholds``
==================================  ========================  ==========================

Sufficient coalitions
^^^^^^^^^^^^^^^^^^^^^

The fourth key, ``sufficient_coalitions``, describes the subsets of criteria required to get into upper categories.
It contains as many items as there are boundaries between categories, *i.e.* as there are categories minus one.

*lincs* only manipulates :math:`U^c \textsf{-} NCS` models for now, so the sufficient coalitions are identical for all categories.
To avoid repetitions in the model files, we use `YAML anchors and references <https://yaml.org/spec/1.2-old/spec.html#id2765878>`_.
All ``*coalitions`` means in the example above is "the same value as the ``&coalitions`` anchor".

Each item in the list has a first attribute, ``kind``, that tells the method used to determine the sufficient coalitions.

If its ``kind`` is ``weights``, then the sufficient coalitions are computed using an MR-sort approach,
as described in our :ref:`conceptual overview <overview-mrsort>`.
In that case, the ``criterion_weights`` attribute is a list of the criteria's weights.
It must contain exactly one element per criterion.

If its ``kind`` is ``roots``, then the sufficient coalitions are listed explicitly as the roots of the upset they form.
This is the generic case for NCS models.
In that case, the ``upset_roots`` attribute is a list of roots, where each root is the list of the zero-based indices of the criteria in that root.

==============================  ================================
Sufficient coalitions ``kind``  Sufficient coalitions attributes
==============================  ================================
``weights``                     ``criterion_weights``
``roots``                       ``upset_roots``
==============================  ================================

Here is another model corresponding to the problem file above, but this time using the ``roots`` kind of sufficient coalitions,
and using different coalitions for the two boundaries (so, no YAML anchor)::

    {{ notebooks["integration-tests/file-formats/file-formats.ipynb"]["cells"][2]["outputs"][0]["text"] | join('    ') }}

"Alternatives" files
--------------------

The last file format used by *lincs* is for the description of alternatives.
It's a CSV file with a header line and one line per alternative.

Like model files, alternatives files are always associated to a problem file.

.. highlight:: text

Here is an example corresponding to the problem above::

    {{ notebooks["integration-tests/file-formats/file-formats.ipynb"]["cells"][3]["outputs"][0]["text"] | join('    ') }}

Its header line contains the names of its columns.
Its first column, ``name``, contains the names of the alternatives.
Its intermediate columns, named after the names of criteria, contain the values of the criteria for each alternative.
Its last column, ``category``, contains the names of the categories in which each alternative is classified.

Values in the ``category`` column can be empty to describe alternatives that are not (yet) classified::

    {{ notebooks["integration-tests/file-formats/file-formats.ipynb"]["cells"][4]["outputs"][0]["text"] | join('    ') }}

.. _user-comments-in-generated-files:

Comments in generated files
---------------------------

When the *lincs* command-line generates a file, it adds a few comment lines (starting with ``#``) at the beginning describing how this file was made.
These comments are informative and can help reproducing results, but they are not part of the file formats.


Generating synthetic data
=========================

The previous section described how to format your data to use it with *lincs*.
As explained in our :ref:`conceptual overview <overview-synthetic-data>`,
you can skip this step and use *lincs* to generate synthetic data.

The parent command to generate synthetic data is ``lincs generate``.
Its sub-commands specify what to generate.
Like all *lincs* commands, they output on the standard output by default,
and you can change that behavior using options to output to files.

About randomness
----------------

Most sub-commands of ``lincs generate`` use pseudo-randomness to generate their output.
By default, the pseudo-random number generator is initialized with a seed based on the current machine, time, *etc.* to favor originality.

When you need reproducibility, you can specify the seed to use with the ``--random-seed`` option.

In all cases, the :ref:`comments <user-comments-in-generated-files>` left by *lincs* in the generated files specify the seed that was used.

.. highlight:: shell

Generating a problem
--------------------

With ``lincs generate classification-problem``, you can generate a classification problem file.
Using its default settings, you just have to pass it the numbers of criteria and categories you want, as you saw in our :doc:`get started guide <get-started>`::

    {{ notebooks["integration-tests/synthetic-data/synthetic-data.ipynb"]["cells"][0]["source"][0] }}

The ``--help`` option on the command-line and our :ref:`reference documentation <ref-cli>` describe the options available to tweak the generated problem.
Most notably:

- ``--denormalized-min-max`` generates problems with pseudo-random ``min_value`` and ``max_value`` for each criterion. By default, they are always set at 0 and 1.
- ``--allow-decreasing-criteria`` chooses pseudo-randomly the ``preference_direction`` of each criterion between ``increasing`` and ``decreasing``. By default, all criteria have ``increasing`` preference direction.

Generating a model
------------------

With ``lincs generate classification-model``, you can generate a classification model file.
Using its default settings, you just have to pass it the problem file you want to use::

    {{ notebooks["integration-tests/synthetic-data/synthetic-data.ipynb"]["cells"][1]["source"][0] }}

For now, *lincs* can only generate MR-Sort models, so the ``--model-type`` option can only take its default value: ``mrsort``.
We expect this could change if we implement the generation of other types of models.

By default, the sum of MR-Sort weights of the criteria is pseudo-random and greater than or equal to 1.
With the ``--mrsort.fixed-weight-sum`` option, you can specify a fixed value for this sum.
This effectively impacts how hard it is for alternatives to get into upper categories.

Generating alternatives
-----------------------

With its default settings, ``lincs generate classified-alternatives`` requires only the problem and model files and the number of alternatives to generate::

    {{ notebooks["integration-tests/synthetic-data/synthetic-data.ipynb"]["cells"][2]["source"][0] }}

This generates 100 random alternatives, and then classifies them according to the model.

By default, no effort is made to balance the number of alternatives in each category.
The ``--max-imbalance`` option can be used to ensure that: it accepts a number between 0 and 1,
and ensures that the number of alternatives in each category differs from the perfectly balanced size by at most this fraction.

For example, when generating 600 alternatives for a model with 3 categories, the perfectly balanced size is 200 alternatives per category.
With ``--max-imbalance 0.2``, the number of alternatives in each category is allowed to differ by at most 20% from that perfectly balanced size,
so each category will have between 160 and 240 alternatives.

Using this option with very selective models can significantly increase the time required to generate the alternatives.
In some cases, *lincs* will even give up when it makes no progress trying to populate categories that are too hard to reach.
In that case, you can either increase the value passed to ``--max-imbalance`` or use a more lenient model.

By default, alternatives are classified exactly according to the given model.
You can introduce noise using the ``--misclassified-count`` option.
After alternatives are generated and classified, this option randomly selects the given number of alternatives and classifies them in other categories.


.. _user-learning-a-model:

Learning a model
================

As you've seen in our get started guide, the basic command to learn a classification model with *lincs* is ``lincs learn classification-model``.
With its default settings, you just have to pass it a problem file and a learning set file (of classified alternatives)::

    {{ notebooks["integration-tests/synthetic-data/synthetic-data.ipynb"]["cells"][3]["source"][0] }}

Its ``--help`` option and our reference documentation give you a list of the numerous options it accepts.

An whole tree of options
------------------------

The first option is ``--model-type``.
It tells *lincs* what type of model you want it to learn, *e.g.* ``mrsort`` for MR-Sort or ``ucncs`` for :math:`U^c \textsf{-} NCS`.
Then, each model type has its own set of options that are valid only for this type of model,
and this pattern goes on to form a tree of options that make sense only on a specific branch.

To capture this reality in a somewhat simple but consistent way, *lincs* uses a dot-separated naming scheme for its options:
option ``--mrsort.strategy`` is a sub-option of ``--model-type mrsort``.
It can accept the value ``weights-profiles-breed``,
and ``--mrsort.weights-profiles-breed.target-accuracy`` is a sub-option of ``--mrsort.strategy weights-profiles-breed``.
The ``model-type`` and ``strategy`` parts are not repeated to reduce verbosity a bit, but this relies on our ability to avoid naming collisions.
Each sub-option name is formed by joining with dots (``.``) the values of the options it depends on.

This pattern is arguably quite verbose, but it's explicit and relatively easy to extend in a backward-compatible manner.

Note that you've already seen an example of this scheme above, at a smaller scale, in ``lincs generate classification-model``,
where ``--mrsort.fixed-weight-sum`` is a sub-option of ``--model-type mrsort``.

Strategies
----------

Some problems can be solved using different methods.
In software, these methods are often called `"strategies" <https://en.wikipedia.org/wiki/Strategy_pattern>`_.
``lincs learn classification-model`` accepts several options named like ``--...strategy`` to let you choose among different methods for a given part of the learning.

A few of them let you choose among only one strategy... but we expect it will change when we implement more.

Available learning (sub-)strategies
-----------------------------------

Examples in this section will reuse the ``problem.yml`` and ``learning-set.csv`` files you have generated in our :doc:`"Get started" guide <get-started>`;
please make sure you have them in your current directory.

Weights, profiles, breed (WPB)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``--mrsort.strategy weights-profiles-breed`` strategy is the default for MR-Sort models.
This methods uses a small population of models, repeating the following three steps:

- improve their MR-Sort weights
- improve their boundary profiles
- breed them to keep the best models and generate new ones

It finally outputs the best model it found.

General options
...............

The size of that population is controlled by the ``--mrsort.weights-profiles-breed.models-count`` option.
Finding the optimal size is a difficult problem.
*lincs* uses a parallel implementation of the WPB loop,
so we recommend you set it to the number of physical CPU cores available on you machine.
Or maybe a small multiple of that number.

The ``--mrsort.weights-profiles-breed.verbose`` option can be used to make *lincs* display information about the progress of the learning.

Termination
...........

The WPB loop terminates when one of the following conditions is met:

- the ``--mrsort.weights-profiles-breed.target-accuracy`` is reached
- the ``--mrsort.weights-profiles-breed.max-duration`` is exceeded: the total duration of the learning is greater than that duration
- the ``--mrsort.weights-profiles-breed.max-duration-without-progress`` is exceeded: the accuracy of the best model so far has not improved for that duration

In all those cases, *lincs* outputs the best model it found so far.

Then, each step is controlled by its own set of options.

"Weights" step
..............

Using ``--mrsort.weights-profiles-breed.weights-strategy linear-program`` (the default and only value for that option),
the "weights" step is actually an optimization, not just an improvement.
That strategy uses a linear program, and lets you choose among several solvers with the ``--mrsort.weights-profiles-breed.linear-program.solver`` option.

By default, it uses GLOP, which is a part of `Google's OR-Tools <https://developers.google.com/optimization/>`_.

Here is an example using the `Alglib <https://www.alglib.net/>`_ solver::

    {{ notebooks["integration-tests/alglib-learning/alglib-learning.ipynb"]["cells"][1]["source"] | join('    ') }}

It should produce a very similar model, with slight numerical differences.

"Profiles" step
...............

The "profiles" step currently only has one strategy (``--mrsort.weights-profiles-breed.profiles-strategy accuracy-heuristic``),
which is controlled by two options.

The first one is a random seed for reproducibility (``--mrsort.weights-profiles-breed.accuracy-heuristic.random-seed``).
The remarks about randomness above also apply here.

The second option lets you use your CUDA-capable GPU for increased performance: ``--mrsort.weights-profiles-breed.accuracy-heuristic.processor``.
Note that *lincs* may be built without GPU support.
This is the case for example on macOS, where CUDA is not supported.
Binary wheels for Linux and Windows do support it though.
You can check with ``lincs info has-gpu``.

Here is an example::

    {{ notebooks["integration-tests/gpu-learning/gpu-learning.ipynb"]["cells"][1]["source"] | join('    ') }}

If you specify the random seed, it will produce the exact same model as when using the CPU;
this is an important feature of *lincs*, that the GPU code has the same behavior as the CPU code.

"Breed" step
............

The "breed" step currently has only one strategy, that simply re-initializes the least accurate models to random ones picked according to the only ``--mrsort.weights-profiles-breed.initialization-strategy`` currently available.
Not much to be said here, but we anticipe this could evolve.

The portion of the population that is reinitialized is controlled by the ``--mrsort.weights-profiles-breed.reinitialize-least-accurate.portion`` option.

SAT-based strategies
^^^^^^^^^^^^^^^^^^^^

.. highlight:: shell

You can also use entirely different approaches using SAT and max-SAT solvers.
The tradeoffs offered by these methods are highlighted in our :ref:`conceptual overview <overview-learning-methods>`.

These strategies let you learn :math:`U^c \textsf{-} NCS` models, so you have to start with ``--model-type ucncs``.
Here are two examples::

    {{ notebooks["integration-tests/sat-learning/sat-learning.ipynb"]["cells"][1]["source"] | join('    ') }}

And::

    {{ notebooks["integration-tests/sat-learning/sat-learning.ipynb"]["cells"][2]["source"] | join('    ') }}

.. highlight:: yaml

They produce a different kind of model, with the sufficient coalitions specified explicitly by their roots::

    {{ notebooks["integration-tests/sat-learning/sat-learning.ipynb"]["cells"][1]["outputs"][0]["text"] | join('    ') }}

Using a model
=============

Classifying alternatives
------------------------

When you finally have a model (learned, generated or hand-crafted),
you can use it to classify alternatives with ``lincs classify problem.yml model.yml alternatives.csv``.

The ``category`` column in the input alternatives file is ignored and may be empty.

Note that the input files will not be modified: the classified alternatives will be printed on the standard output
or written in the file specified by ``--output-alternatives``.

Computing a classification accuracy
-----------------------------------

Similarly, you can use ``lincs classification-accuracy problem.yml model.yml learning-set.csv`` to compute the accuracy of a model on a learning set.

In that case, the ``category`` column must be populated as it serves as a reference to compute the accuracy.

That command displays the number of alternatives that were correctly classified and the total number of alternatives in the learning set.

Visualizing a model and alternatives
------------------------------------

And you can use ``lincs visualize classification-model problem.yml model.yml`` to create a graphical representation of a model (a ``.png`` file),
and its ``--alternatives`` and ``--alternatives-count`` options to add alternatives to the graph.
You've seen an example in our "Get started" guide.


What's next?
============

You now know pretty much everything you need to use *lincs*.
You may find some additional details in our :doc:`reference documentation <reference>`.
Feel free to reach out to us if you have any question or feedback, as said at the top of the :doc:`Readme <index>`.
