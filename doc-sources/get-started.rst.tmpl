.. Copyright 2023 Vincent Jacques

===========
Get started
===========


Get *lincs*
===========

We provide binary wheels for *lincs* on Linux, Windows and macOS for x86_64 processors,
so running ``pip install lincs --only-binary lincs`` should be enough on those systems.

We generally recommend you use ``pip`` in a virtual environment (``python -m venv``) or directly ``pipx`` to install any package, including *lincs*.
Recent Ubuntu systems will even enforce that, by `refusing to install PyPI packages <https://itsfoss.com/externally-managed-environment/>`_ in the "externally managed" default environment.

If you're on a platform for which we don't make wheels, you'll need to build *lincs* from sources.
We don't recommend you do that, because it can be a lot of work.
If you really want to go that route, you may want to start by reading the `GitHub Actions workflow <https://github.com/MICS-Lab/lincs/blob/main/.github/workflows/distribute.yml>`_ we use to build the binary wheels.
You'll probably start by trying ``pip install lincs``, see what dependencies are missing, install them and iterate from there.
If you end up modifying *lincs* to make it work on your platform, we kindly ask you to contribute your changes back to the project.

.. _start-command-line:

Start using *lincs*' command-line interface
===========================================

If you're a Jupyter user, you can `download the notebook <get-started.ipynb>`_ this section is based on.

.. highlight:: text

The command-line interface is the easiest way to get started with *lincs*, starting with ``{{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][0]["source"][0] }}``, which should output something like::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][0]["outputs"][0]["text"] | join('    ') }}

It's organized into sub-commands, the first one being ``generate``, to generate synthetic pseudo-random data.

*lincs* is designed to handle real-world data, but it's often easier to start with synthetic data to get familiar with the tooling and required file formats.
Synthetic data is described in our :ref:`conceptual overview documentation <overview-synthetic-data>`.

.. highlight:: shell

So, start by generating a classification problem with 4 criteria and 3 categories::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][1]["source"][0] }}

.. highlight:: yaml

The generated ``problem.yml`` should look like::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][1]["outputs"][0]["text"] | join('    ') }}

You can edit this file to change the criteria names, the number of categories, *etc.* as long as you keep the same format.
That format is explained in details in our :ref:`user guide <user-file-problem>`.
The concept of "classification problem" is described in our :ref:`conceptual overview documentation <overview-about-classification>`.
Note that to keep this "Get Started" simple, we only consider the most basic kind of criteria: real-valued,
with normalized minimal and maximal values, and increasing preference direction.
There are many other kinds of criteria, and you can read about them in our user guide.

If you want a human-readable explanation of the problem, you can use::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][2]["source"][0] }}

It will tell you something like::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][2]["outputs"][0]["text"] | join('    ') }}

.. highlight:: shell

Then generate an NCS classification model::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][3]["source"][0] }}

.. highlight:: yaml

It should look like::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][3]["outputs"][0]["text"] | join('    ') }}

The file format, including the ``*coalitions`` YAML reference, is documented in our :ref:`user guide <user-file-ncs-model>`.

.. highlight:: shell

You can visualize it using::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][4]["source"][1] }}

It should output something like:

.. image:: get-started/model.png
    :alt: Model visualization
    :align: center

The model format is quite generic to ensure *lincs* can evolve to handle future models,
so you may want to get a human-readable description of a model, including wether it's an MR-Sort or Uc-NCS model, using::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][5]["source"][0] }}

It should output something like::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][5]["outputs"][0]["text"] | join('    ') }}

And finally generate a set of classified alternatives::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][6]["source"][0] }}

The file format is documented in our :ref:`reference documentation <ref-file-alternatives>`.

@todo(Feature, later) Should we provide utilities to split a set of alternatives into a training set and a testing set?
Currently we suggest generating two sets from a synthetic model, but for real-world data it could be useful to split a single set.
Then we'll need to think about the how the ``--max-imbalance`` option interacts with that feature.

.. highlight:: text

It should start with something like this, and contain 1000 alternatives::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][6]["outputs"][0]["text"] | join('    ') }}

.. highlight:: shell

You can visualize its first five alternatives using::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][7]["source"][1] }}

It should output something like:

.. image:: get-started/alternatives.png
    :alt: Alternatives visualization
    :align: center

.. highlight:: shell

You now have a (synthetic) learning set. You can use it to train a new model::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][8]["source"][0] }}

.. highlight:: yaml

The trained model has the same structure as the original (synthetic) model because they are both MR-Sort models for the same problem.
The learning set doesn't contain all the information from the original model,
and the trained model was reconstituted from this partial information,
so it is numerically different::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][8]["outputs"][0]["text"] | join('    ') }}

If the training is effective, the resulting trained model should however behave closely to the original one.
To see how close a trained model is to the original one, you can reclassify a testing set.

.. highlight:: shell

First, generate a testing set from the original model::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][9]["source"][0] }}

.. highlight:: shell

Then ask the trained model to classify it::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][10]["source"][0] }}

.. highlight:: shell

There are a few differences between the original testing set and the reclassified one::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][10]["source"][1] }}

.. highlight:: diff

That command should show a few alternatives that are not classified the same way by the original and the trained model::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][10]["outputs"][0]["text"] | join('    ') }}

.. highlight:: shell

You can also measure the classification accuracy of the trained model on that testing set::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][11]["source"][0] }}

.. highlight:: text

It should be close to 100%::

    {{ notebooks["doc-sources/get-started/get-started.ipynb"]["cells"][11]["outputs"][0]["text"] | join('    ') }}


What now?
=========

If you haven't done so yet, we recommend you now read our :doc:`conceptual overview documentation <conceptual-overview>`.

Keep in mind that we've only demonstrated the default learning approach in this guide.
See our :doc:`user guide <user-guide>` for more details.

.. @todo(Documentation, later) Add an intermediate document, a case study, that shows a realistic use case.

Once you're comfortable with the concepts and tooling, you can use a learning set based on real-world data and train a model that you can use to classify new real-world alternatives.
